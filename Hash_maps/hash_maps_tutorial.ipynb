{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tutorial on HashMap\n",
    " Data structures help us in representing and efficiently manipulating the data associated with real world problems.\n",
    "\n",
    " Let's work on such a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem Scenario\n",
    "\n",
    "In a class of students, store heights for each student.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in itself is very simple. We have the data of heights of each student. We want to store it so that next time someone asks for height of a student, we can easily return the value. But how can we store these heights?\n",
    "\n",
    "Obviously we can use a database and store these values. But, let's say we don't want to do that for now. We want to use a data structure to store these values as part of our program. For the sake of simplicity, our problem is limited to storing heights of students. But you can certainly imagine scenarios where you have to store such `key-value` pairs and later on when someone gives you a `key`, you can efficiently return the corrresponding `value`.\n",
    "\n",
    "The class diagram for HashMaps would look something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashMap:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_entries = 0\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        pass\n",
    "    \n",
    "    def get(self, key):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays, Linked-Lists, Queues and Stacks\n",
    "\n",
    "We can try any of the above data structures to see if possible to implement a hashmap. \n",
    "<br><br>\n",
    "An array to store the names and another to store the heights would work. However, in a worst case scenario we would need to traverse the aentire array to match the name with the requested height leading to `O(n)` time complexity and when using a sorted array at best `O(log(n))` complexity.\n",
    "<br><br>\n",
    "A linked list would indeed work however it still would need traversing hence this would not increase the lookup to constant time.\n",
    "<br><br>\n",
    "Queues and Stacks would surely increase the lookup for the cases when getting the oldest and newest elemts respectively to `O(1)`, however this would not work for other elements as we would need to traverse the queues and the stacks.\n",
    "<br><br>\n",
    "Looking again at arrays if we could use an array index associated with a `key` (in this specific case a name) to look up the `value` (in this case height). i.e `arr[3]` then this would be a constant look up time. `The only problem now is how to turn strings or any other data structure to an array index??`\n",
    "\n",
    "#### Hashing functions\n",
    "Hashing functions in turn help us to answer the above question and therefore make our hashmap functional with constant lookup time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(string):\n",
    "    # we can use sum corresponding ASCII values of string and use it as the hash value\n",
    "    # ord(character) determines ASCII value of a particular character\n",
    "    hash_code = 0\n",
    "    for char in string:\n",
    "         hash_code += ord(char)\n",
    "\n",
    "    return hash_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "print(hash_function(\"abcd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above hashing function is not a good one. This is because it gives the same answer for `abcd` and `bcad`,leading to coliision. A good hashing function should avoid collision. Honestly in reality differrent types of keys require different hashing functions. i.e. hash functions for strings will be different from hash functions for intergers and different still for objects of our own created classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Functions for Strings\n",
    "For a string like `abcde` an effective solution is to treat this as number of prime number base `p`.  This is to say:\n",
    "For a number e.g. `578` can be represented in base 10 number system as $$5*10^2+7*10^1+8*10^0$$\n",
    "\n",
    "Now for our string we can similarly write; $$a*p^4+b*10^3+c*10^2+d*10^1+e*10^0$$ However,we replace the letters with their corresponding ASCII values. This methood of implementing hash functions for strings is among the best functions among a well researched area. Prime numbers are ideal since they offer a good distribution. Most common prime numbers used are `31` and `37`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence using the above algorithm we can get a corresponding interger value for each string key and **use it as an array index** of an array e.g. `bucket_array`. Each entry in this array is called a `bucket` and the index to store a bucket is a `bucket_index`. The `bucket_array` can be visualised as shown below.\n",
    "\n",
    "<img style=\"float: center;height:500px\" src=\"bucket0.png\"><br>\n",
    "\n",
    "Defining our class with these details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashMap:\n",
    "\n",
    "    def __init__(self,initial_size=10) -> None:\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.num_entries = 0\n",
    "        self.p = 37\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        pass\n",
    "    \n",
    "    def get(self, key):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "    \n",
    "    def get_bucket_index(self,key):\n",
    "        return self.get_hash_code(key)\n",
    "\n",
    "    def get_hash_code(self,key):\n",
    "        # to ensure it is a key\n",
    "        key = str(key)\n",
    "        hash_code = 0\n",
    "        # first coeffecient represented below as self.p^0=1\n",
    "        coeffecient = 1\n",
    "\n",
    "        for character in key:\n",
    "            hash_code += ord(character)*coeffecient\n",
    "            coeffecient *= self.p\n",
    "\n",
    "        return hash_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5204554\n"
     ]
    }
   ],
   "source": [
    "hash_map = HashMap()\n",
    "\n",
    "bucket_index = hash_map.get_bucket_index(\"abcd\")\n",
    "print(bucket_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054002\n"
     ]
    }
   ],
   "source": [
    "hash_map = HashMap()\n",
    "\n",
    "bucket_index = hash_map.get_bucket_index(\"bcda\")\n",
    "print(bucket_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that our hash function is not causing collision as the simple one discussed above. However the `bucket_index` are *way huge* and **creating such large arrays will be a space complexity issue which is not viable**. A way out of this is to use a compression function to compress the values outputed above and hence create reasonably sized arrays.<br><br>\n",
    "A very good,simple and effective compression function can be the `mod len(array)` utilizing the `modulu operator %`, which returns the remainder of one number divided by the other.<br><br>\n",
    "So, if we have an array of size 10, we can be sure that modulo of any number with 10 will be less than 10, allowing it to fit into our bucket array. You can visualize the `bucket array` again as shown in the figure below, in which the `bucket_index` is generated by the string key:\n",
    "\n",
    "<img  src=\"bucket1.png\"><br>\n",
    "\n",
    "**Note that here we are storing the string key and corresponding numeric value in a Node**. \n",
    "Because of how modulo operator works, instead of creating a new function, we can write the logic for compression function in our `get_hash_code()` function itself.\n",
    "\n",
    "https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/modular-multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashMap:\n",
    "\n",
    "    def __init__(self,initial_size=10) -> None:\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.num_entries = 0\n",
    "        self.p = 31\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        pass\n",
    "    \n",
    "    def get(self, key):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "    \n",
    "    def get_bucket_index(self,key):\n",
    "        return self.get_hash_code(key)\n",
    "\n",
    "    def get_hash_code(self,key):\n",
    "        # to ensure it is a key\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        hash_code = 0\n",
    "        # first coeffecient represented below as self.p^0=1\n",
    "        coeffecient = 1\n",
    "\n",
    "        for character in key:\n",
    "            hash_code += ord(character)*coeffecient\n",
    "            hash_code = hash_code % num_buckets #compress hash code\n",
    "            coeffecient *= self.p \n",
    "            coeffecient = coeffecient % num_buckets #compress coeffecient\n",
    "\n",
    "        return hash_code #one last compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Check the bucket_index for two different strings made with same set of characters\n",
    "hash_map = HashMap()\n",
    "\n",
    "bucket_index = hash_map.get_bucket_index(\"one\")\n",
    "print(bucket_index)\n",
    "\n",
    "bucket_index = hash_map.get_bucket_index(\"neo\")\n",
    "print(bucket_index)                                  # Collision might occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collision Handling\n",
    "As discussed earlier, when two different inputs produce the same output, then we have a collision. Our implementation of `get_hash_code()` function is satisfactory. However, because we are using compression function, we are prone to collisions. **Remember, that a key will always be unique. But the bucket_index generated by two different keys can be the same.**\n",
    "\n",
    "**Consider the following scenario** - We have a bucket array of length 10 and we get two different hash codes for two different inputs, say 355, and 1095. Even though the hash codes are different in this case, the bucket index will be same because of the way we have implemented our compression function. Such scenarios where multiple entries want to go to the same bucket are very common. So, we introduce some logic to handle collisions.\n",
    "\n",
    "There are two popular ways of handling this collision.\n",
    "1. **Separate Chaining** - In this technique we use the same bucket to store multiple objects with the same `bucket_index`. The `bucket` in this case will store a linked-list of key-value pairs inside the `bucket_array`. Every `bucket` in the `bucket_array` will store it's own separate chain of linked-list nodes.\n",
    "<br><br>\n",
    "\n",
    "2. **Open Addressing** - In open addressing we;\n",
    "   \n",
    "   * If, after getting the `bucket_index` the object is empty;we store the object in that particular bucket.\n",
    "   * If, the bucket is not empty we find another `bucket_index` by using another function to modify the current `hash_code` to give a new code. This process of finding another `hash_code` is called **probing**. A few probing techniques include - linear probing,quadratic probing, or double hashing.\n",
    "\n",
    "   Separate chaining is a goof technique which we will implement.\n",
    "\n",
    "   <img src=\"bucket2.png\"/> <br><br>\n",
    "\n",
    "   Implementing put() and get() using the separate chaining method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedListNode:\n",
    "    \n",
    "    def __init__(self,key,value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "\n",
    "class HashMap:\n",
    "\n",
    "    def __init__(self,initial_size=10) -> None:\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.num_entries = 0\n",
    "        self.p = 31\n",
    "\n",
    "    '''Separate chaining:\n",
    "       In case of collision, the `put()` function uses the same bucket to store a linked list of key-value pairs.\n",
    "       Every bucket (or bucket_index) will have it's own separate chain of linked-lists nodes\n",
    "    '''\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        bucket_index = self.get_bucket_index(key) #getting a bucket_index as per the given key\n",
    "        head = self.bucket_array[bucket_index] # setting head to reference the start of the linked-list in the particular bucket\n",
    "\n",
    "        new_node = LinkedListNode(key,value) #creating a new node for the newly passed key.\n",
    "\n",
    "        #check if key is already present in map and then update its value.\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                head.value = value\n",
    "                return #end loop\n",
    "            head = head.next\n",
    "    \n",
    "        '''\n",
    "        If the key is a new one, hence not found in the chain (LinkedList), then following two cases arise:\n",
    "         1. The key has generated a new bucket_index\n",
    "         2. The key has generated an existing bucket_index. \n",
    "            This event is a Collision, i.e., two different keys have same bucket_index.\n",
    "\n",
    "        In both the cases, we will prepend the new node (key, value) at the beginning (head) of the chain (LinkedList).\n",
    "        Remember that each `bucket` at position `bucket_index` is actually a chain (LinkedList) with 1 or more nodes.  \n",
    "        '''\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        new_node.next = head\n",
    "        self.bucket_array[bucket_index] = new_node #prepending new_node to head of linked-list chain\n",
    "        self.num_entries += 1\n",
    "\n",
    "\n",
    "    def get(self, key):\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        \n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                return head.value\n",
    "            head = head.next\n",
    "\n",
    "        return None  \n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "    \n",
    "    def get_bucket_index(self,key):\n",
    "        return self.get_hash_code(key)\n",
    "\n",
    "    def get_hash_code(self,key):\n",
    "        # to ensure it is a key\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        hash_code = 0\n",
    "        # first coeffecient represented below as self.p^0=1\n",
    "        coeffecient = 1\n",
    "\n",
    "        for character in key:\n",
    "            hash_code += ord(character)*coeffecient\n",
    "            hash_code = hash_code % num_buckets #compress hash code\n",
    "            coeffecient *= self.p \n",
    "            coeffecient = coeffecient % num_buckets #compress coeffecient\n",
    "\n",
    "        return hash_code #one last compression\n",
    "\n",
    "     # Helper function to see the hashmap\n",
    "    def __repr__(self):\n",
    "        output = \"\\nLet's view the hash map:\"\n",
    "\n",
    "        node = self.bucket_array\n",
    "        for bucket_index, node in enumerate(self.bucket_array):\n",
    "            if node is None:\n",
    "                output += '\\n[{}] '.format(bucket_index)\n",
    "            else:\n",
    "                output += '\\n[{}]'.format(bucket_index)\n",
    "                while node is not None:\n",
    "                    output += ' ({} , {}) '.format(node.key, node.value)\n",
    "                    if node.next is not None:\n",
    "                        output += ' --> '\n",
    "                    node = node.next\n",
    "                    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 4\n",
      "one: 1\n",
      "neo: 11\n",
      "three: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Let's view the hash map:\n",
       "[0] \n",
       "[1] \n",
       "[2] (neo , 11)  -->  (one , 1) \n",
       "[3] \n",
       "[4] \n",
       "[5] \n",
       "[6] (three , 3)  -->  (two , 2) \n",
       "[7] \n",
       "[8] \n",
       "[9] "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the collision resolution technique\n",
    "hash_map = HashMap()\n",
    "\n",
    "hash_map.put(\"one\", 1)\n",
    "hash_map.put(\"two\", 2)\n",
    "hash_map.put(\"three\", 3)          # Collision: The key \"three\" will generate the same bucket_index as that of the key \"two\"\n",
    "hash_map.put(\"neo\", 11)           # Collision: The key \"neo\" will generate the same bucket_index as that of the key \"one\"\n",
    "\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "print(\"one: {}\".format(hash_map.get(\"one\")))\n",
    "print(\"neo: {}\".format(hash_map.get(\"neo\")))\n",
    "print(\"three: {}\".format(hash_map.get(\"three\")))\n",
    "\n",
    "hash_map                          # call to the helper function to see the hashmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time complexity and Rehashing\n",
    "Arrays were used to implement hashmaps sinmce they offer $O(1)$ time complexity for put and get operations.\n",
    "\n",
    "*Note: In case of arrays, put is simply `arr[i] = 5` and get is `height = arr[i]`*\n",
    "\n",
    "##### 1. Put Operation\n",
    "* In the put operation, we first figure out the bucket index. Calculating the hash code to figure out the bucket index takes some time.\n",
    "* After that, we go to the bucket index and in the worst case we traverse the linked list to find out if the key is already present or not. This also takes some time.\n",
    "\n",
    "To analyze the time complexity for any algorithm as a function of the input size `n`, we first have to determine what our input is. In this case, we are putting and getting key-value pairs. So, these entries i.e. key-value pairs are our input. Therefore, our `n` is number of such key-value pair entries.\n",
    "\n",
    "*Note: time complexity is always determined in terms of input size and not the actual amount of work that is being done independent of input size. That \"independent amount of work\" will be constant for every input size so we disregard that.*\n",
    "\n",
    "* In case of our hash function, the computation time for hash code depends on the size of each string. Compared to number of entries (which we always consider to be very high e.g. in the order of  105 ) the length of each string can be considered to be very small. Also, most of the strings will be around the same size when compared to this high number of entries. Hence, we can ignore the hash computation time in our analysis of time complexity.\n",
    "  \n",
    "* Now, the entire time complexity essentialy depends on the linked list traversal. In the worst case, all entries would go to the same bucket index and our linked list at that index would be huge. Therefore, the time complexity in that scenario would be  $ùëÇ(ùëõ)$ . However, hash functions are wisely chosen so that this does not happen.\n",
    "\n",
    "`On average, the distribution of entries is such that if we have n entries and b buckets, then each bucket does not have more than n/b key-value pair entries.`\n",
    "\n",
    "Therefore, because of our choice of hash function we can assume the complexity to be $O(\\dfrac{n}{b})$.\n",
    "This number which determines the `load` on our bucket array `n/b` is known as `load factor`.\n",
    "Generally, we try to keep our load factor around or less than 0.7. This essentially means that if we have a bucket array of size 10, then the number of key-value pair entries will not be more than 7.\n",
    "\n",
    "**What happens when we get more entries and the value of our load factor crosses 0.7?**\n",
    "\n",
    "In that scenario, we must increase the size of our bucket array. Also, we must recalculate the bucket index for each entry in the hash map.\n",
    "\n",
    "*Note: the hash code for each key present in the bucket array would still be the same. However, because of the compression function, the bucket index will change.* \n",
    "\n",
    "Therefore, we need to `rehash` all the entries in our hash map. This is known as `Rehashing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get and Delete operation\n",
    "\n",
    "Time complexity for get and delete operation is;\n",
    "\n",
    " The solution follows the same logic. We assume a constant time of operation for generating the hash code (bucket-index) for a given key. Ignore this constant time. Similarly, we can refer to the head of linked list at generated bucket-index in $O(1)$ time. Next, we might have to traverse the the linked list in the worst-case scenario, making the time complexity as  $ùëÇ(\\dfrac{ùëõ}{ùëè})$.Note that we do not reduce the size of bucket array in delete operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Consideration for Time complexity of  `put` and `get` Operation\n",
    "\n",
    "**Note:** Theoretically worst case   time complexity of `put` and `get` operations of a HashMap can be $O(\\dfrac{n}{b}) \\approx O(n)$, when $b << n$. However, our hashing functions are sophisticated enough that in real-life we easily avoid collisions and never hit $O(n)$.Rather, for the most part, we can safely assume that the time complexity of put and get operations will be $O(1)$.\n",
    "\n",
    "Therefore, when you are asked to solve any practice problem involving HashMaps, assume the worst case time complexity for put and get operations to be $O(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rehashing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedListNode:\n",
    "\n",
    "    def __init__(self,key,value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "class HashMap:\n",
    "\n",
    "    def __init__(self,initial_size=15):\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.p = 31\n",
    "        self.num_entries = 0\n",
    "        self.load_factor = 0.7\n",
    "\n",
    "    def put(self,key,value):\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "\n",
    "        new_node = LinkedListNode(key,value)\n",
    "\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        #if key is existing we update it's value\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                head.value = value\n",
    "                return\n",
    "            head = head.next\n",
    "        #if key is not present or existing we prepend new node to head of bucket linked-list    \n",
    "        head = self.bucket_array[bucket_index]\n",
    "        new_node.next = head\n",
    "        self.bucket_array[bucket_index] = new_node\n",
    "        self.num_entries += 1\n",
    "\n",
    "        #check for load factor\n",
    "        current_load_factor = self.num_entries / len(self.bucket_array)\n",
    "        if current_load_factor > self.load_factor:\n",
    "            self.num_entries = 0\n",
    "            self._rehash()\n",
    "\n",
    "    def get(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                return head.value\n",
    "            head = head.next\n",
    "        return None\n",
    "        \n",
    "    def get_bucket_index(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        return bucket_index\n",
    "    \n",
    "    def get_hash_code(self, key):\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        current_coefficient = 1\n",
    "        hash_code = 0\n",
    "        for character in key:\n",
    "            hash_code += ord(character) * current_coefficient\n",
    "            hash_code = hash_code % num_buckets                       # compress hash_code\n",
    "            current_coefficient *= self.p\n",
    "            current_coefficient = current_coefficient % num_buckets   # compress coefficient\n",
    "        return hash_code % num_buckets                                # one last compression before returning\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "\n",
    "    # rehashing function\n",
    "    def _rehash(self):\n",
    "        old_bucket_array = self.bucket_array\n",
    "        old_num_buckets = len(self.bucket_array)\n",
    "        num_buckets = 2 * old_num_buckets\n",
    "        self.bucket_array = [None for _ in range (num_buckets)]\n",
    "\n",
    "        for head in old_bucket_array:\n",
    "            while head is not None:\n",
    "                key = head.key\n",
    "                value = head.value\n",
    "                self.put(key,value)  #using our existing put method to populate the new 'bucket_array'\n",
    "                head = head.next\n",
    "\n",
    "     # Helper function to see the hashmap\n",
    "    def __repr__(self):\n",
    "        output = \"\\nLet's view the hash map:\"\n",
    "\n",
    "        node = self.bucket_array\n",
    "        for bucket_index, node in enumerate(self.bucket_array):\n",
    "            if node is None:\n",
    "                output += '\\n[{}] '.format(bucket_index)\n",
    "            else:\n",
    "                output += '\\n[{}]'.format(bucket_index)\n",
    "                while node is not None:\n",
    "                    output += ' ({} , {}) '.format(node.key, node.value)\n",
    "                    if node.next is not None:\n",
    "                        output += ' --> '\n",
    "                    node = node.next\n",
    "                    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 4\n",
      "one: 1\n",
      "neo: 11\n",
      "three: 3\n",
      "size: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Let's view the hash map:\n",
       "[0] \n",
       "[1] \n",
       "[2] (one , 1)  -->  (neo , 11) \n",
       "[3] \n",
       "[4] \n",
       "[5] \n",
       "[6] (two , 2)  -->  (three , 3) \n",
       "[7] \n",
       "[8] \n",
       "[9] "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Rehashing\n",
    "\n",
    "# We have reduced the size of the hashmap array to increase the load factor (> 0.7) \n",
    "# and hence trigger the rehash() function\n",
    "hash_map = HashMap(5)                        \n",
    "\n",
    "hash_map.put(\"one\", 1)\n",
    "hash_map.put(\"two\", 2)\n",
    "hash_map.put(\"three\", 3)\n",
    "hash_map.put(\"neo\", 11)\n",
    "\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "\n",
    "print(\"one: {}\".format(hash_map.get(\"one\")))\n",
    "print(\"neo: {}\".format(hash_map.get(\"neo\")))\n",
    "print(\"three: {}\".format(hash_map.get(\"three\")))\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "hash_map                          # call to the helper function to see the hashmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Operation.\n",
    "\n",
    "Delete operation implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedListNode:\n",
    "    \n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "class HashMap:\n",
    "    \n",
    "    def __init__(self, initial_size = 15):\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.p = 31\n",
    "        self.num_entries = 0\n",
    "        self.load_factor = 0.7\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "\n",
    "        new_node = LinkedListNode(key, value)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "\n",
    "        # check if key is already present in the map, and update it's value\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                head.value = value\n",
    "                return\n",
    "            head = head.next\n",
    "\n",
    "        # key not found in the chain --> create a new entry and place it at the head of the chain\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        new_node.next = head\n",
    "        self.bucket_array[bucket_index] = new_node\n",
    "        self.num_entries += 1\n",
    "        \n",
    "        # check for load factor\n",
    "        current_load_factor = self.num_entries / len(self.bucket_array)\n",
    "        if current_load_factor > self.load_factor:\n",
    "            self.num_entries = 0\n",
    "            self._rehash()\n",
    "        \n",
    "    def get(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                return head.value\n",
    "            head = head.next\n",
    "        return None\n",
    "        \n",
    "    def get_bucket_index(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        return bucket_index\n",
    "    \n",
    "    def get_hash_code(self, key):\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        current_coefficient = 1\n",
    "        hash_code = 0\n",
    "        for character in key:\n",
    "            hash_code += ord(character) * current_coefficient\n",
    "            hash_code = hash_code % num_buckets                       # compress hash_code\n",
    "            current_coefficient *= self.p\n",
    "            current_coefficient = current_coefficient % num_buckets   # compress coefficient\n",
    "        return hash_code % num_buckets                                # one last compression before returning\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "\n",
    "    def _rehash(self):\n",
    "        old_num_buckets = len(self.bucket_array)\n",
    "        old_bucket_array = self.bucket_array\n",
    "        num_buckets = 2 * old_num_buckets\n",
    "        self.bucket_array = [None for _ in range(num_buckets)]\n",
    "\n",
    "        for head in old_bucket_array:\n",
    "            while head is not None:\n",
    "                key = head.key\n",
    "                value = head.value\n",
    "                self.put(key, value)         # we can use our put() method to rehash\n",
    "                head = head.next\n",
    "\n",
    "    def delete(self,key):\n",
    "        #first get the bucket index of supplied key\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "        #set a reference to the bucket reffered to by the bucket_index\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        #check if reference is none or not\n",
    "        previous = None\n",
    "        while head is not None:\n",
    "        #if not none find the key corresponding to the given key and delete it by setting it to None\n",
    "            if head.key == key:\n",
    "                if previous is None:\n",
    "                    self.bucket_array[bucket_index] = head.next\n",
    "                else:\n",
    "                    previous.next = head.next\n",
    "                self.num_entries -= 1\n",
    "                return\n",
    "            else:  #iteration point\n",
    "                previous = head\n",
    "                head = head.next\n",
    "\n",
    "        # Helper function to see the hashmap\n",
    "    def __repr__(self):\n",
    "        output = \"\\nLet's view the hash map:\"\n",
    "\n",
    "        node = self.bucket_array\n",
    "        for bucket_index, node in enumerate(self.bucket_array):\n",
    "            if node is None:\n",
    "                output += '\\n[{}] '.format(bucket_index)\n",
    "            else:\n",
    "                output += '\\n[{}]'.format(bucket_index)\n",
    "                while node is not None:\n",
    "                    output += ' ({} , {}) '.format(node.key, node.value)\n",
    "                    if node.next is not None:\n",
    "                        output += ' --> '\n",
    "                    node = node.next\n",
    "                    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 4\n",
      "one: 1\n",
      "neo: 11\n",
      "three: 3\n",
      "size: 4\n",
      "None\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Test delete operation\n",
    "hash_map = HashMap(7)\n",
    "\n",
    "hash_map.put(\"one\", 1)\n",
    "hash_map.put(\"two\", 2)\n",
    "hash_map.put(\"three\", 3)\n",
    "hash_map.put(\"neo\", 11)\n",
    "\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "\n",
    "print(\"one: {}\".format(hash_map.get(\"one\")))\n",
    "print(\"neo: {}\".format(hash_map.get(\"neo\")))\n",
    "print(\"three: {}\".format(hash_map.get(\"three\")))\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "hash_map                          # call to the helper function to see the hashmap\n",
    "\n",
    "\n",
    "hash_map.delete(\"one\")\n",
    "hash_map                          # call to the helper function to see the hashmap\n",
    "\n",
    "print(hash_map.get(\"one\"))\n",
    "print(hash_map.size())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edd1d3549cf072e5f1c96c28c5126be4dddecf6bd9bc18231197754c991085f8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('datastructures')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
